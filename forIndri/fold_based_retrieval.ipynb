{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in/out of the passage based dataset\n",
    "dataset = \"/home/thiziri/Documents/DOCTORAT/OSIRIM/projets/iris/PROJETS/WEIR/data/title_only/robust\"  # passages_ranking_indri/Robust/MZ_data/concatenated/5\"\n",
    "# \"/home/thiziri/Documents/DOCTORAT/SOFT/MatchZoo_latest1/data/AP88/passageRetrieval_qrels/concatenated\"\n",
    "output_index = \"/home/thiziri/Documents/DOCTORAT/COLLECTION/Indri_index/Robust_title\"  # Robust_passages\"\n",
    "# \"/home/thiziri/Documents/DOCTORAT/OSIRIM/projets/iris/PROJETS/WEIR/collections/Indri_Index/passages_based/Robust\"\n",
    "# \"/home/thiziri/Desktop/tst\"\n",
    "doc_templet = \"\"\"<DOC>\n",
    "    <DOCNO>doc_num</DOCNO>\n",
    "    <TEXT>\n",
    "    doc_text\n",
    "    </TEXT>\n",
    "</DOC>\"\"\"\n",
    "indri = \"/home/thiziri/Documents/DOCTORAT/SOFT/INDRI/indri-5.11\"  # \"/home/thiziri/Documents/DOCTORAT/OSIRIM/logiciels/indri-5.11/\"\n",
    "run_out = \"/home/thiziri/Documents/DOCTORAT/OSIRIM/projets/iris/PROJETS/WEIR/RUNS/2ndYear/indri_runs/title_only\"  # passages_based_robust\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xml tools\n",
    "from xml.etree import ElementTree\n",
    "from xml.dom import minidom\n",
    "\n",
    "def prettify(elem):\n",
    "    \"\"\"\n",
    "    Return a pretty-printed XML string for the Element.\n",
    "    \"\"\"\n",
    "    rough_string = ElementTree.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    return reparsed.toprettyxml(indent=\"  \").replace(\"<?xml version=\\\"1.0\\\" ?>\\n\", \"\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "read unique values from column n in the file f\n",
    "\"\"\"\n",
    "def read_values(f, n):\n",
    "    inf = open(f, \"r\")\n",
    "    lines = inf.readlines()\n",
    "    result = []\n",
    "    for x in lines:\n",
    "        result.append(x.split()[n])\n",
    "    inf.close()\n",
    "    return set(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create dataset of the concatenated documents to index them with INDRI\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "\n",
    "for fold in tqdm(os.listdir(dataset)):\n",
    "    # construct temporary corpus\n",
    "    if not os.path.exists(os.path.join(output_index, fold)):\n",
    "        os.mkdir(os.path.join(output_index, fold))\n",
    "    queries = {}\n",
    "    docs = []\n",
    "    with open(os.path.join(os.path.join(output_index, fold), \"corpus.txt\"), 'w') as out_file:\n",
    "        with open(os.path.join(os.path.join(dataset, fold), \"corpus.txt\"), 'r') as in_file:\n",
    "            for line in in_file:\n",
    "                tokens = line.strip().split()\n",
    "                if tokens[0].isalnum():\n",
    "                    queries[tokens[0]] = \" \".join(tokens[1:])  # get queries texts\n",
    "                else:\n",
    "                    doc_id = tokens[0]  # .split('_')[0]  # we could have the same doc_id with diff contents for diff queries\n",
    "                    docs.append(doc_templet.replace(\"doc_num\", doc_id).replace(\"doc_text\", \" \".join(tokens[1:-1])))\n",
    "        out_file.write('\\n'.join(docs))  # write the documents mini-collection\n",
    "        c_root = ET.Element(\"parameters\")\n",
    "        for q_id in queries:  # create the corresponding query-file for indri\n",
    "            # print(q_id, q_txt)\n",
    "            c_query = ET.SubElement(c_root, \"query\")\n",
    "            type_ = ET.SubElement(c_query, \"type\")\n",
    "            type_.text = \"indri\"\n",
    "            num = ET.SubElement(c_query, \"number\")\n",
    "            num.text = q_id\n",
    "            text = ET.SubElement(c_query, \"text\")\n",
    "            text.text = queries[q_id]\n",
    "        with open(os.path.join(os.path.join(output_index, fold), \"queries.xml\"), 'w') as q_out:\n",
    "            q_out.write(prettify(c_root))\n",
    "    break  # only one fold, because file \"corpus.txt\" contains all queries and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r100%|██████████| 1/1 [00:12<00:00, 12.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# index and query with INDRI:\n",
    "for fold in tqdm(os.listdir(output_index)):\n",
    "    if os.path.isdir(os.path.join(output_index, fold)):\n",
    "        # create the index parameter file:\n",
    "        with open(os.path.join(os.path.join(output_index, fold), \"index_parameter.xml\"), 'w') as q_out:\n",
    "            c_root = ET.Element(\"parameters\")\n",
    "            mem = ET.SubElement(c_root, \"memory\")\n",
    "            mem.text = \"2000m\"\n",
    "            index = ET.SubElement(c_root, \"index\")\n",
    "            index.text = os.path.join(os.path.join(output_index, fold), \"index\")\n",
    "            stem = ET.SubElement(c_root, \"stemmer\")\n",
    "            stem_name = ET.SubElement(stem, \"name\")\n",
    "            stem_name.text = \"krovetz\"\n",
    "            corp = ET.SubElement(c_root, \"corpus\")\n",
    "            corp_path = ET.SubElement(corp, \"path\")\n",
    "            corp_path.text = os.path.join(os.path.join(output_index, fold), \"corpus.txt\")\n",
    "            corp_class = ET.SubElement(corp, \"class\")\n",
    "            corp_class.text = \"trectext\"\n",
    "            field = ET.SubElement(c_root, \"field\")\n",
    "            name = ET.SubElement(corp, \"name\")\n",
    "            name.text = \"TEXT\"\n",
    "            q_out.write(prettify(c_root))\n",
    "                \n",
    "        # perform indexing AND ranking:\n",
    "        os.chdir(indri)\n",
    "        cmd = \"buildindex/IndriBuildIndex \" + os.path.join(os.path.join(output_index, fold), \"index_parameter.xml\")\n",
    "        print(\"Indexing ...\")\n",
    "        os.system(cmd)\n",
    "        \n",
    "        cmd = \"runquery/IndriRunQuery \" + os.path.join(os.path.join(output_index, \n",
    "                                                                    fold), \n",
    "                                                       \"queries.xml\") + \" -count=1000 -index=\" + os.path.join(os.path.join(output_index, \n",
    "                                                                                                                           fold), \n",
    "                                                                                                              \"index\") + \\\n",
    "              \" -trecFormat=true -baseline=okapi,k1:1.2,b:0.75,k3:7 > \" + os.path.join(run_out, \"okapi_k1_1.2_b_0.75_k3_7\" + fold)\n",
    "        print(\"Retrieving ...\")\n",
    "        os.system(cmd)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-cff6478684dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc_id\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# aggregate the different passages of a same document: needed for documents represented by passages ONLY \n",
    "\n",
    "from collections import defaultdict\n",
    "from numpy import average as avg\n",
    "import operator\n",
    "\n",
    "for file in tqdm(os.listdir(run_out)):\n",
    "    uniques = defaultdict(dict)\n",
    "    f = open(os.path.join(run_out, file), 'r')\n",
    "    for line in f.readlines():\n",
    "        if len(line.strip()) > 0:\n",
    "            tokens = line.strip().split()\n",
    "            doc_id, q_id = tokens[2].split('_')\n",
    "            score = float(tokens[4])\n",
    "            if doc_id not in uniques[q_id]:\n",
    "                uniques[q_id][doc_id] = [score] \n",
    "            else:\n",
    "                uniques[q_id][doc_id].append(score)\n",
    "    # average scores:\n",
    "    scores = defaultdict(dict)\n",
    "    for q in tqdm(uniques):\n",
    "        for doc_id in uniques[q]:\n",
    "            scores[q][doc_id] = round(avg(uniques[q][doc_id]), 5)\n",
    "    # write results:\n",
    "    with open(os.path.join(run_out, file + \"_aggreg\"), 'w') as out:\n",
    "        sorted_scores = scores  # dict(sorted(scores.items(), key=lambda kv: kv[1]))\n",
    "        for q in tqdm(sorted_scores):\n",
    "            sorted_docs = sorted(sorted_scores[q].items(), key=operator.itemgetter(1), reverse=True)\n",
    "            rank = range(1, len(sorted_docs)+1)\n",
    "            iter_rank = iter(rank)\n",
    "            q_results = [\"{q} Q0 {d} {i} {s} indri\".format(q=q, d=e[0], i=next(iter_rank), \n",
    "                                                           s=e[1]) for e in sorted_docs]\n",
    "            out.write('\\n'.join(q_results) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
